<h3 align="center">
Columbia Deep Learning For Computer Vision Project</h3>

The dataset can be downloaded by following the UniSign Repo. My self collects data and the checkpoint can be found at  https://drive.google.com/drive/folders/1O26t0k77xqs1Cop7PNIIuKrYQN161z1A
Run the script under the script/ folder for fine-tuning.
Run realtime.py for real-time translation.

## üëç Acknowledgement
The codebase of Uni-Sign is adapted from [GFSLT-VLP](https://github.com/zhoubenjia/GFSLT-VLP), while the implementations of the pose/temporal encoders are derived from [CoSign](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiao_CoSign_Exploring_Co-occurrence_Signals_in_Skeleton-based_Continuous_Sign_Language_Recognition_ICCV_2023_paper.pdf). We sincerely appreciate the authors of CoSign for personally sharing their code üôè. \
We are also grateful for the following projects our Uni-Sign arise from:
* ü§ü[SSVP-SLT](https://github.com/facebookresearch/ssvp_slt): a excellent sign language translation framework! 
* üèÉÔ∏è[MMPose](https://github.com/open-mmlab/mmpose): an open-source toolbox for pose estimation.
* ü§†[FUNASR](https://github.com/modelscope/FunASR): a high-performance speech-to-text toolkit.


## üìë Citation
If you find Uni-Sign useful for your research and applications, please cite using this BibTeX:
```
@article{li2025uni,
  title={Uni-Sign: Toward Unified Sign Language Understanding at Scale},
  author={Li, Zecheng and Zhou, Wengang and Zhao, Weichao and Wu, Kepeng and Hu, Hezhen and Li, Houqiang},
  journal={arXiv preprint arXiv:2501.15187},
  year={2025}
}
```
