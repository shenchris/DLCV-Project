<h3 align="center">
Columbia Deep Learning For Computer Vision Project</h3>



## ğŸ‘ Acknowledgement
The codebase of Uni-Sign is adapted from [GFSLT-VLP](https://github.com/zhoubenjia/GFSLT-VLP), while the implementations of the pose/temporal encoders are derived from [CoSign](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiao_CoSign_Exploring_Co-occurrence_Signals_in_Skeleton-based_Continuous_Sign_Language_Recognition_ICCV_2023_paper.pdf). We sincerely appreciate the authors of CoSign for personally sharing their code ğŸ™. \
We are also grateful for the following projects our Uni-Sign arise from:
* ğŸ¤Ÿ[SSVP-SLT](https://github.com/facebookresearch/ssvp_slt): a excellent sign language translation framework! 
* ğŸƒï¸[MMPose](https://github.com/open-mmlab/mmpose): an open-source toolbox for pose estimation.
* ğŸ¤ [FUNASR](https://github.com/modelscope/FunASR): a high-performance speech-to-text toolkit.


## ğŸ“‘ Citation
If you find Uni-Sign useful for your research and applications, please cite using this BibTeX:
```
@article{li2025uni,
  title={Uni-Sign: Toward Unified Sign Language Understanding at Scale},
  author={Li, Zecheng and Zhou, Wengang and Zhao, Weichao and Wu, Kepeng and Hu, Hezhen and Li, Houqiang},
  journal={arXiv preprint arXiv:2501.15187},
  year={2025}
}
```
